Ethical Notes on Fairness and Accuracy:

When the protected variables are excluded from this analysis, the average correctness of the calculated results lowers. From 163/199 correct when the variables are included, to 152.8/199 correct without, this would suggest that we should include the protected variables in the data in order to obtain more accurate results. 

Disparate impact analysis yields the most divisive results. When the protected variables are included, majority Black communities are disproportionately under-predicted – that is, they receive a distinctly lower positive categorization than all the other groups. Without the protected variables, they still receive the lowest positive rate, but the gap is notably smaller. This suggests we should exclude the protected variables from the calculation, since the disparate impact has smaller differences between the racial groups when it is left out. 

The false positive rates are about the same, but they are distributed differently in the two cases. When the protected variables are included, majority White communities barely received false positives, about 1%, but when the protected variables were excluded, the false positive rate changed to 10%. Conversely, majority Black communities went from about 10% false positives to 8%, a minor change, but more notable that the other two groups which remained static. This would advocate to retain the protected variables in the data, as the false positive rate is higher without. 

The predictive parity lowered across the board when the protected variable was removed, although not by a large margin, with the exception of majority Hispanic communities. When the protected variables were removed from the data, the predictive parity lowered noticeably, almost 5%. The other groups had lowered parity, but only by a few percentage points. However, in both cases predominantly Asian communities were shown to be the least likely to be correctly categorized, and majority Black communities were the most likely to be correctly categorized into the positive group. In this case, removing the protected attribute increased the fairness of the predictive parity across the groups, as the numbers were closer to each other. This would advocate for removing the protected variables from the data. 

From a fairness standpoint, the clear answer is to remove the protected variables from the data. From an accuracy standpoint, it’s better to retain the protected variables. Overall, I would value fairness over complete accuracy, even if it means sacrificing some accuracy, and choose to remove the protected variables from the data. The disproportionate targeting of racial groups is both illegal and unethical, more so than an imperfect algorithm, which will have some level of inaccuracy regardless.
